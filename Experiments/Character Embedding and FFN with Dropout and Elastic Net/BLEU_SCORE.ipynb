{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c2h73RTJYvE",
        "outputId": "33573859-5f41-4c83-ab32-9aadb84b7c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The nltk version is 3.8.1.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "print('The nltk version is {}.'.format(nltk.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0coIs47IK9C",
        "outputId": "fb48b51f-35bf-451c-b09c-5c192a600f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.1\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "faPpzn1SIYan"
      },
      "outputs": [],
      "source": [
        "# File paths\n",
        "Amharic = \"/content/drive/My Drive/Bleu_ScoreElasticnet/Amharic.txt\"\n",
        "English = \"/content/drive/My Drive/Bleu_ScoreElasticnet/English.txt\"\n",
        "\n",
        "# Read the files\n",
        "try:\n",
        "    with open(Amharic, 'r', encoding='utf-8') as am_file, open(English, 'r', encoding='utf-8') as en_file:\n",
        "        amharic_sentences = am_file.read().splitlines()\n",
        "        english_sentences = en_file.read().splitlines()\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"File not found: {e}\")\n",
        "    raise\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\"()\\[\\]{}\\\\/-]', '', text) #remove any character that is not a letter, digit, whitespace, punctuation, bracket, or slash\n",
        "\n",
        "def handle_unicode(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "def expand_contractions(text):\n",
        "    contractions = {\n",
        "        \"n't\": \" not\",\n",
        "        \"'s\": [\" is\", \" was\"],  # Can be either \"is\" or \"was\"\n",
        "        \"'m\": \" am\",\n",
        "        \"'re\": \" are\",\n",
        "        \"'ll\": \" will\",\n",
        "        \"'ve\": \" have\",\n",
        "        \"'d\": [\" would\", \" had\"]  # Can be either \"would\" or \"had\"\n",
        "    }\n",
        "    for contraction, expansion in contractions.items():\n",
        "        if isinstance(expansion, list):\n",
        "            for exp in expansion:\n",
        "                text = text.replace(contraction, exp)\n",
        "        else:\n",
        "            text = text.replace(contraction, expansion)\n",
        "    return text\n",
        "\n",
        "def remove_excess_whitespace(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "# Preprocess Amharic sentences\n",
        "def preprocess_amharic(text):\n",
        "    # Remove any non-Amharic characters (keeping spaces, Amharic/Arabic numbers, and Amharic punctuation)\n",
        "    text = re.sub(r'[^\\u1200-\\u137F\\u1361-\\u1368\\u1369-\\u1371\\u0030-\\u0039\\s\\[\\]{}\\\\/-]', '', text)\n",
        "\n",
        "    # Remove excess whitespace\n",
        "    text = remove_excess_whitespace(text)\n",
        "    return list(text)  # Return a list of characters\n",
        "\n",
        "# Preprocess English sentences\n",
        "def preprocess_english(text):\n",
        "    text = handle_unicode(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_excess_whitespace(text)\n",
        "    return list(text)  # Return a list of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JetAADRfgV2W",
        "outputId": "2b44c1b1-644b-48a2-ea49-7dedb8b04b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amharic minimum sentence length: 1\n",
            "Amharic maximum sentence length: 892\n",
            "English minimum sentence length: 1\n",
            "English maximum sentence length: 1323\n",
            "Overall minimum sentence length: 1\n",
            "Overall maximum sentence length: 1323\n",
            "Number of pairs before filtering: 99987\n",
            "Number of pairs after filtering: 99987\n",
            "99851 total pairs\n",
            "63904 training pairs\n",
            "15976 validation pairs\n",
            "19971 test pairs\n"
          ]
        }
      ],
      "source": [
        "# Preprocess sentences and calculate lengths\n",
        "am_lengths = []\n",
        "en_lengths = []\n",
        "text_pairs = []\n",
        "for am_sentence, en_sentence in zip(amharic_sentences, english_sentences):\n",
        "    am_processed = preprocess_amharic(am_sentence)\n",
        "    en_processed = preprocess_english(en_sentence)\n",
        "    am_length = len(am_processed)  # list of Amharic characters\n",
        "    en_length = len(en_processed)  # list of English characters\n",
        "\n",
        "    # Filter out zero-length sentences\n",
        "    if am_length > 0 and en_length > 0:\n",
        "        am_lengths.append(am_length)\n",
        "        en_lengths.append(en_length)\n",
        "        text_pairs.append((''.join(am_processed), ''.join(en_processed)))  # Join characters back into a string\n",
        "# Calculate min and max lengths for each language\n",
        "am_min_length = min(am_lengths)\n",
        "am_max_length = max(am_lengths)\n",
        "en_min_length = min(en_lengths)\n",
        "en_max_length = max(en_lengths)\n",
        "\n",
        "# Calculate overall min and max lengths\n",
        "min_length = min(am_min_length, en_min_length)\n",
        "max_length = max(am_max_length, en_max_length)\n",
        "\n",
        "print(f\"Amharic minimum sentence length: {am_min_length}\")\n",
        "print(f\"Amharic maximum sentence length: {am_max_length}\")\n",
        "print(f\"English minimum sentence length: {en_min_length}\")\n",
        "print(f\"English maximum sentence length: {en_max_length}\")\n",
        "print(f\"Overall minimum sentence length: {min_length}\")\n",
        "print(f\"Overall maximum sentence length: {max_length}\")\n",
        "\n",
        "# Filter sentences based on length\n",
        "filtered_pairs = []\n",
        "filtered_am_lengths = []\n",
        "filtered_en_lengths = []\n",
        "\n",
        "for (am_sentence, en_sentence), am_length, en_length in zip(text_pairs, am_lengths, en_lengths):\n",
        "    if min_length <= am_length <= max_length and min_length <= en_length <= max_length:\n",
        "        am_sentence = \"[start] \" + am_sentence + \" [end]\"\n",
        "        en_sentence = \"[start] \" + en_sentence + \" [end]\"\n",
        "        filtered_pairs.append((am_sentence, en_sentence))\n",
        "        filtered_am_lengths.append(am_length)\n",
        "        filtered_en_lengths.append(en_length)\n",
        "\n",
        "# Print the number of pairs before and after filtering\n",
        "print(f\"Number of pairs before filtering: {len(text_pairs)}\")\n",
        "print(f\"Number of pairs after filtering: {len(filtered_pairs)}\")\n",
        "\n",
        "# Calculate combined lengths and handle single occurrences\n",
        "combined_lengths = [am + en for am, en in zip(filtered_am_lengths, filtered_en_lengths)]\n",
        "counts = Counter(combined_lengths)\n",
        "# Filter out lengths that occur only once\n",
        "filtered_pairs = [pair for pair, length in zip(filtered_pairs, combined_lengths) if counts[length] > 1]\n",
        "filtered_lengths = [length for length in combined_lengths if counts[length] > 1]\n",
        "\n",
        "# Split the data into train, validation, and test sets with stratification\n",
        "train_pairs, test_pairs = train_test_split(filtered_pairs, test_size=0.2, random_state=42, stratify=filtered_lengths)\n",
        "train_lengths, test_lengths = train_test_split(filtered_lengths, test_size=0.2, random_state=42, stratify=filtered_lengths)\n",
        "\n",
        "train_pairs, val_pairs = train_test_split(train_pairs, test_size=0.2, random_state=42, stratify=train_lengths)\n",
        "train_lengths, val_lengths = train_test_split(train_lengths, test_size=0.2, random_state=42, stratify=train_lengths)\n",
        "\n",
        "# Print the size of each set\n",
        "print(f\"{len(filtered_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP3gi7OFgj9U",
        "outputId": "609f93b6-2017-4387-bd18-5ef877304169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max tokens in Amharic: 105\n",
            "Max tokens in English: 141\n",
            "95th percentile of token counts in Amharic: 31\n",
            "95th percentile of token counts in English: 46\n"
          ]
        }
      ],
      "source": [
        "# Custom standardization function\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.lower(input_string)\n",
        "\n",
        "# Tokenize and count frequency\n",
        "def tokenize_and_count(texts):\n",
        "    all_tokens = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        all_tokens.extend(tokens)\n",
        "    return Counter(all_tokens)\n",
        "\n",
        "train_am_texts = [pair[0] for pair in train_pairs]\n",
        "train_en_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "am_token_freq = tokenize_and_count(train_am_texts)\n",
        "en_token_freq = tokenize_and_count(train_en_texts)\n",
        "\n",
        "# Calculate and print max tokens for both languages\n",
        "am_max_tokens = max(len(text.split()) for text in train_am_texts)\n",
        "en_max_tokens = max(len(text.split()) for text in train_en_texts)\n",
        "\n",
        "print(f\"Max tokens in Amharic: {am_max_tokens}\")\n",
        "print(f\"Max tokens in English: {en_max_tokens}\")\n",
        "\n",
        "# Calculate the 95th percentile of token counts\n",
        "am_token_counts = [len(text.split()) for text in train_am_texts]\n",
        "en_token_counts = [len(text.split()) for text in train_en_texts]\n",
        "am_95th_percentile = int(np.percentile(am_token_counts, 95))\n",
        "en_95th_percentile = int(np.percentile(en_token_counts, 95))\n",
        "print(f\"95th percentile of token counts in Amharic: {am_95th_percentile}\")\n",
        "print(f\"95th percentile of token counts in English: {en_95th_percentile}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Pl9aikIyGF",
        "outputId": "5e08647d-ea5b-4af6-be9c-537c84d206fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Amharic sequence length: 1053\n",
            "Max English sequence length: 1733\n",
            "Amharic vocabulary size: 319\n",
            "English vocabulary size: 55\n"
          ]
        }
      ],
      "source": [
        "def get_max_sequence_length(texts):\n",
        "    return max([len(text) for text in texts])\n",
        "\n",
        "# Preprocess and get max lengths\n",
        "am_texts = [' '.join(preprocess_amharic(text)) for text, _ in train_pairs + val_pairs + test_pairs]\n",
        "en_texts = [' '.join(preprocess_english(text)) for _, text in train_pairs + val_pairs + test_pairs]\n",
        "\n",
        "max_am_sequence_length = get_max_sequence_length(am_texts)\n",
        "max_en_sequence_length = get_max_sequence_length(en_texts)\n",
        "\n",
        "print(f\"Max Amharic sequence length: {max_am_sequence_length}\")\n",
        "print(f\"Max English sequence length: {max_en_sequence_length}\")\n",
        "\n",
        "# TextVectorization layers with max sequence lengths\n",
        "am_vectorization = layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_am_sequence_length,\n",
        "    standardize=custom_standardization,\n",
        "    split='character'\n",
        ")\n",
        "en_vectorization = layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_en_sequence_length,\n",
        "    standardize=custom_standardization,\n",
        "    split='character'\n",
        ")\n",
        "\n",
        "# Adapt vectorization layers on training data\n",
        "am_vectorization.adapt(train_am_texts)\n",
        "en_vectorization.adapt(train_en_texts)\n",
        "\n",
        "# Vocabulary sizes\n",
        "amharic_vocab_size = len(am_vectorization.get_vocabulary())\n",
        "english_vocab_size = len(en_vectorization.get_vocabulary())\n",
        "print(f\"Amharic vocabulary size: {amharic_vocab_size}\")\n",
        "print(f\"English vocabulary size: {english_vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G6T-ba3DdwwF"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "d_model = 256 #512 embed_dim\n",
        "latent_dim = 512 #1024 #dif\n",
        "num_heads = 8\n",
        "num_layers = 5\n",
        "dropout_rate = 0.3\n",
        "# Dataset formatting function\n",
        "def format_dataset(am, en):\n",
        "    am = am_vectorization(am)\n",
        "    en = en_vectorization(en)\n",
        "    return ({\"encoder_inputs\": am, \"decoder_inputs\": en[:, :-1]}, en[:, 1:])\n",
        "\n",
        "# Function to create dataset\n",
        "def make_dataset(pairs, batch_size):\n",
        "    am_texts, en_texts = zip(*pairs)\n",
        "    am_texts = list(am_texts)\n",
        "    en_texts = list(en_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((am_texts, en_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset.shuffle(1024).prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
        "\n",
        "# Create the datasets\n",
        "train_ds = make_dataset(train_pairs, batch_size)\n",
        "val_ds = make_dataset(val_pairs, batch_size)\n",
        "test_ds = make_dataset(test_pairs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g82-nmHTI9Qd",
        "outputId": "9a356b97-fa0d-4aa2-81f9-a8d86ccf5430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model successfully loaded and compiled!\n"
          ]
        }
      ],
      "source": [
        "# Define necessary custom objects for loading the model\n",
        "\n",
        "elastic_net = tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Multi-Head Attention\n",
        "class CustomMultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(CustomMultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.3)  \n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        original_q = tf.reshape(tf.transpose(q, perm=[0, 2, 1, 3]), (batch_size, -1, self.d_model))\n",
        "        output = self.layernorm(output + original_q)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Point-Wise Feed Forward Network with dropout and layer normalization\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu', kernel_regularizer=elastic_net),\n",
        "        tf.keras.layers.Dense(d_model, kernel_regularizer=elastic_net),\n",
        "        tf.keras.layers.Dropout(0.3), \n",
        "        tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return tf.cast(pos, tf.float32) * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "    \n",
        "    # Apply sin to even indices in the array; 2i\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    \n",
        "    # Apply cos to odd indices in the array; 2i+1\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.3):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = CustomMultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.3):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = CustomMultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = CustomMultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, amharic_vocab_size, rate=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(amharic_vocab_size, d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += positional_encoding(seq_len, self.d_model)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, english_vocab_size, rate=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(english_vocab_size, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += positional_encoding(seq_len, self.d_model)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, amharic_vocab_size, english_vocab_size, rate=0.3):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, amharic_vocab_size, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, english_vocab_size, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(english_vocab_size)\n",
        "\n",
        "        # Store the parameters for serialization\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.amharic_vocab_size = amharic_vocab_size\n",
        "        self.english_vocab_size = english_vocab_size\n",
        "        self.dropout_rate = rate\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if isinstance(inputs, dict):\n",
        "            inp, tar = inputs['encoder_inputs'], inputs['decoder_inputs']\n",
        "        else:\n",
        "            inp, tar = inputs\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, combined_mask, dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output, attention_weights\n",
        "\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions, _ = self(inputs, training=True)\n",
        "            loss = self.compiled_loss(targets, predictions, regularization_losses=self.losses)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        self.compiled_metrics.update_state(targets, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        inputs, targets = data\n",
        "        predictions, _ = self(inputs, training=False)\n",
        "        self.compiled_loss(targets, predictions, regularization_losses=self.losses)\n",
        "        self.compiled_metrics.update_state(targets, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff,\n",
        "            \"amharic_vocab_size\": self.amharic_vocab_size,\n",
        "            \"english_vocab_size\": self.english_vocab_size,\n",
        "            \"rate\": self.dropout_rate,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Custom learning rate schedule\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(tf.cast(self.d_model, tf.float32)) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"d_model\": self.d_model,\n",
        "            \"warmup_steps\": self.warmup_steps\n",
        "        }\n",
        "\n",
        "# Function to create masks\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "    dec_padding_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)\n",
        "    dec_target_padding_mask = tf.cast(tf.math.equal(tar, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "# Loss and Accuracy\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Load the model with custom objects\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/ElasticNet/SavedModel', custom_objects={\n",
        "    'Transformer': Transformer,\n",
        "    'CustomSchedule': CustomSchedule,\n",
        "    'Encoder': Encoder,\n",
        "    'Decoder': Decoder,\n",
        "    'EncoderLayer': EncoderLayer,\n",
        "    'DecoderLayer': DecoderLayer,\n",
        "    'CustomMultiHeadAttention': CustomMultiHeadAttention\n",
        "})\n",
        "\n",
        "# Create Transformer instances\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=latent_dim,\n",
        "    amharic_vocab_size=amharic_vocab_size,\n",
        "    english_vocab_size=english_vocab_size,\n",
        "    rate=dropout_rate\n",
        ")\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model\n",
        "transformer.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Assuming you have a batch of data (batch_encoder_inputs, batch_decoder_inputs)\n",
        "batch_encoder_inputs = tf.random.uniform((batch_size, max_am_sequence_length), dtype=tf.int64, minval=0, maxval=amharic_vocab_size)\n",
        "batch_decoder_inputs = tf.random.uniform((batch_size, max_en_sequence_length), dtype=tf.int64, minval=0, maxval=english_vocab_size)\n",
        "\n",
        "# Pass the data through the model\n",
        "_ = transformer({'encoder_inputs': batch_encoder_inputs, 'decoder_inputs': batch_decoder_inputs})\n",
        "\n",
        "print(\"The model successfully loaded and compiled!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naXNIvgYJSa7",
        "outputId": "d04cbb29-4237-4d1d-c882-b1062bc60d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum BLEU score: 0.9636\n",
            "Average BLEU score: 0.3727\n",
            "\n",
            "Sample Translations and BLEU Scores:\n",
            "\n",
            "Sample 1:\n",
            "Amharic Sentence: [start] ሆኖም የምናሴ ልጅ፣ የማኪር ልጅ፣ የጊልያድ ልጅ፣ የሄፌር ልጅ ሰለጰአድ ሴቶች እንጂ ወንዶች ልጆች አልነበሩትም፤ የሴቶች ልጆቹም ስም ማህላ፣ ኖኅ፣ ሆግላ፣ ሚልካ እና ቲርጻ ነበር። [end]\n",
            "Reference English Sentence: but zelophehad the son of hepher, the son of gilead, the son of machir, the son of manasseh, did not have sons, only daughters, and these were the names of his daughters: mahlah, noah, hoglah, milcah, and tirzah. [end]\n",
            "Predicted English Sentence: [start] but zelophehad the son of gilead, the son of machir, the son of manasseh, was not only of the daughters of manasseh, but the names of the daughters of manasseh were mahlah, noah, hoglah, milcah, and tirzah.\n",
            "BLEU Score: 0.5101\n",
            "\n",
            "Sample 2:\n",
            "Amharic Sentence: [start] ያህዌ የጻድቃንን መንገድ ያውቃልና፤ [end]\n",
            "Reference English Sentence: for jehovah is aware of the way of the righteous, [end]\n",
            "Predicted English Sentence: [start] for jehovah is the way of the path of the path of the wicked\n",
            "BLEU Score: 0.2575\n",
            "\n",
            "Sample 3:\n",
            "Amharic Sentence: [start] ጥሬውን በውኃም የበሰለውን አትብሉ፥ ነገር ግን ከራሱ ከጭኑ ከሆድ ዕቃው ጋር በእሳት የተጠበሰውን ብሉት። [end]\n",
            "Reference English Sentence: eat not of it raw, nor sodden at all with water, but roast with fire; his head with his legs, and with the purtenance thereof. [end]\n",
            "Predicted English Sentence: [start] neither shalt thou not hearken to their own kind, but with his kind, but with his kind, and his legs, and with his legs, and with his legs, and with the fire.\n",
            "BLEU Score: 0.1610\n",
            "\n",
            "Sample 4:\n",
            "Amharic Sentence: [start] ግን ፍልስጤም ነፃ አውጪ ድርጅት ከፍተኛ ባለሥልጣን ለታይም እንደገለጸው የጡረታ አበል ይከፍሏታል እንጂ ከዚህ ውጭ የሚሆን ነገር የለም። [end]\n",
            "Reference English Sentence: but a senior p.l.o. official tells time, \"they will pay her a pension, and that is it.\" [end]\n",
            "Predicted English Sentence: [start] but the american official tells time, \"they will pay a pension, and that is it.\"\n",
            "BLEU Score: 0.5425\n",
            "\n",
            "Sample 5:\n",
            "Amharic Sentence: [start] ምናልባት። [end]\n",
            "Reference English Sentence: i am afraid it might. [end]\n",
            "Predicted English Sentence: [start] i am afraid of this thing.\n",
            "BLEU Score: 0.1931\n"
          ]
        }
      ],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    input_seq = tf.expand_dims(input_seq, 0)\n",
        "    start_token = en_vectorization([\"[start]\"])[0][0].numpy()\n",
        "    end_token = en_vectorization([\"[end]\"])[0][0].numpy()\n",
        "    output_seq = tf.expand_dims([start_token], 0)\n",
        "\n",
        "    for i in range(max_en_sequence_length):  # Iterate up to the max sequence length for the target (English)language\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_seq, output_seq)\n",
        "        predictions, _ = model(input_seq, output_seq, training=False,\n",
        "                               enc_padding_mask=enc_padding_mask,\n",
        "                               look_ahead_mask=combined_mask,\n",
        "                               dec_padding_mask=dec_padding_mask)\n",
        "        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1).numpy()[0][0]\n",
        "        if predicted_id == end_token:\n",
        "            break\n",
        "        output_seq = tf.concat([output_seq, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    return output_seq[0].numpy()\n",
        "\n",
        "# Detokenization function\n",
        "def detokenize(sequence, is_amharic=False):\n",
        "    if is_amharic:\n",
        "        vocab = am_vectorization.get_vocabulary()\n",
        "    else:\n",
        "        vocab = en_vectorization.get_vocabulary()\n",
        "    return ' '.join([vocab[token] for token in sequence if token != 0 and token < len(vocab)])\n",
        "\n",
        "def calculate_bleu_scores(test_ds, num_samples=3000):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_scores = []\n",
        "    sample_sentences = []\n",
        "\n",
        "    for batch in test_ds.take(num_samples // batch_size + 1):\n",
        "        inputs, targets = batch\n",
        "\n",
        "        # Decode input sequences\n",
        "        input_sequences = inputs['encoder_inputs'].numpy()\n",
        "        for i, input_seq in enumerate(input_sequences):\n",
        "            if len(bleu_scores) >= num_samples:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                predicted_sequence = decode_sequence(input_seq)\n",
        "                predicted_en_sentence = detokenize(predicted_sequence)\n",
        "\n",
        "                # Get the target sentence\n",
        "                target_sequence = targets[i].numpy()\n",
        "                target_en_sentence = detokenize(target_sequence)\n",
        "\n",
        "                reference = [target_en_sentence.split()]\n",
        "                candidate = predicted_en_sentence.split()\n",
        "                score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "                bleu_scores.append(score)\n",
        "\n",
        "                # Get the original Amharic sentence\n",
        "                am_sentence = detokenize(input_seq, is_amharic=True)\n",
        "                sample_sentences.append((am_sentence, target_en_sentence, predicted_en_sentence, score))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sentence {len(bleu_scores)}: {str(e)}\")\n",
        "\n",
        "    return bleu_scores, sample_sentences\n",
        "\n",
        "# Calculate BLEU scores\n",
        "num_samples = 3000 \n",
        "bleu_scores, sample_sentences = calculate_bleu_scores(test_ds, num_samples)\n",
        "\n",
        "# Print results\n",
        "max_bleu_score = np.max(bleu_scores)\n",
        "average_bleu_score = np.mean(bleu_scores)\n",
        "print(f\"Maximum BLEU score: {max_bleu_score:.4f}\")\n",
        "print(f\"Average BLEU score: {average_bleu_score:.4f}\")\n",
        "\n",
        "# Display 5 sample sentences with BLEU scores\n",
        "print(\"\\nSample Translations and BLEU Scores:\")\n",
        "for i, (am_sentence, en_sentence, predicted_sentence, score) in enumerate(sample_sentences[:5]):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Amharic Sentence: {am_sentence}\")\n",
        "    print(f\"Reference English Sentence: {en_sentence}\")\n",
        "    print(f\"Predicted English Sentence: {predicted_sentence}\")\n",
        "    print(f\"BLEU Score: {score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
